{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9dfQ-_UhAAcZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c3fd8c6-9d02-4c23-8b21-a1229cace320"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fpdf in /usr/local/lib/python3.10/dist-packages (1.7.2)\n",
            "\u001b[31mERROR: Invalid requirement: '-'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q gpt-2-simple\n",
        "!pip install fpdf\n",
        "!pip install diffusers -q\n",
        "!pip install accelerate - q\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rj_JlN1RAU35",
        "outputId": "9f52ee4a-4e18-404d-9ec4-201ffa60aaa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 3.47Git/s]                                                     \n",
            "Fetching encoder.json: 1.05Mit [00:00, 3.20Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 3.60Git/s]                                                   \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:12, 41.3Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 2.59Git/s]                                               \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 4.51Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 3.67Mit/s]\n"
          ]
        }
      ],
      "source": [
        "gpt2.download_gpt2(model_name=\"124M\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN1D9HWgAVkl",
        "outputId": "baf30da9-df43-467a-e182-89b90bb4e38e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "gpt2.mount_gdrive()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gp0nTJbDAhfi"
      },
      "outputs": [],
      "source": [
        "file_name = 'All Speaches.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dGsayKdlBgjP"
      },
      "outputs": [],
      "source": [
        "gpt2.copy_file_from_gdrive(file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "10eonyb1Bkva",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7384527f-d768-4da1-a6c6-479a5b3b57cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint checkpoint/run1/model-101\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 82295 tokens\n",
            "Training...\n",
            "[110 | 1084.44] loss=1.61 avg=1.61\n",
            "Saving checkpoint/run1/model-110\n",
            "[120 | 2264.27] loss=1.65 avg=1.63\n",
            "Saving checkpoint/run1/model-120\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/saver.py:1067: remove_checkpoint (from tensorflow.python.checkpoint.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[130 | 3426.99] loss=1.45 avg=1.57\n",
            "Saving checkpoint/run1/model-130\n",
            "interrupted\n",
            "Saving checkpoint/run1/model-131\n"
          ]
        }
      ],
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='124M',\n",
        "              steps=1000,\n",
        "              restore_from='latest',\n",
        "              run_name='run1',\n",
        "              print_every=10,\n",
        "              sample_every=200,\n",
        "              save_every=10,\n",
        "              )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "fKd3OFJ7CDht"
      },
      "outputs": [],
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CQjgDRegikfV"
      },
      "outputs": [],
      "source": [
        "gpt2.copy_checkpoint_from_gdrive(run_name='run1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "k50UD8F2itrP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5863519-3c35-45e3-fe88-3f7a2a1b0599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint checkpoint/run1/model-90\n"
          ]
        }
      ],
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name='run1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7vuqKuDRjDZ",
        "outputId": "22dbee48-f88f-49fa-c4fd-7c5f1cfeafaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How many words per line? 6\n",
            "Write the first line here (using the same number of words entered above) I like to eat breakfast pizza\n",
            "How many lines? 10\n",
            "How crazy do you want the book to be? (mild, moderate, spicy) moderate\n",
            "Rhyming scheme? (y/n)y\n"
          ]
        }
      ],
      "source": [
        "Word_line = int(input(\"How many words per line? \"))\n",
        "Prompt = input(\"Write the first line here (using the same number of words entered above) \")\n",
        "number_lines = int(input(\"How many lines? \"))\n",
        "Randomness = input(\"How crazy do you want the book to be? (mild, moderate, spicy) \")\n",
        "book_length = int((Word_line*number_lines)*1.25)\n",
        "Rhyming = input(\"Rhyming scheme? (y/n)\")\n",
        "if Randomness == \"mild\":\n",
        "  temperature = 0.5\n",
        "elif Randomness == \"moderate\":\n",
        "  temperature = 0.75\n",
        "else:\n",
        "  temperature = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxSuulkLT5Pt",
        "outputId": "6c68f2b4-e245-4a18-8572-f02e7b7d3db5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"The stormy clouds sent the rats running for the exits. The only thing left of them was the smell of blood.\\nDazed, the rats sprinted towards the tent before they could even make it to the tent itself. It was a long day in the city, and the city was littered with the smell of bodies. It wasn't long until the smell finally left the tent and the dogs bolted.\"]\n"
          ]
        }
      ],
      "source": [
        "output_file = 'childrens_book.txt'\n",
        "book_length = int(book_length)\n",
        "generated_samples = gpt2.generate(sess,\n",
        "              prefix = Prompt,\n",
        "              length=book_length,\n",
        "              top_k = 40,\n",
        "              temperature=temperature,\n",
        "              batch_size=1,\n",
        "              top_p = 0.95,\n",
        "              return_as_list = True\n",
        "              )\n",
        "print(generated_samples)\n",
        "\n",
        "generated_book = '\\n'.join(generated_samples)\n",
        "\n",
        "with open(output_file, 'w') as file:\n",
        "    file.write(generated_book)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split text to create pages\n",
        "\n",
        "import random\n",
        "\n",
        "def split_text(input_text, words_per_segment):\n",
        "    words = input_text.split()\n",
        "    segments = [words[i:i + words_per_segment] for i in range(0, len(words), words_per_segment)]\n",
        "    return segments\n",
        "\n",
        "input_file = 'childrens_book.txt'\n",
        "output_file_prefix = 'output_segment_'\n",
        "words_per_segment = random.randint(20,30)\n",
        "\n",
        "with open(input_file, 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "text_segments = split_text(content, words_per_segment)\n",
        "\n",
        "\n",
        "for i, segment in enumerate(text_segments):\n",
        "    output_file = f'{output_file_prefix}{i + 1}.txt'\n",
        "    with open(output_file, 'w') as file:\n",
        "        file.write(' '.join(segment))\n",
        "\n",
        "    print(f\"Segment {i + 1} saved to {output_file}\")\n",
        "\n",
        "seg_count = len(text_segments)\n",
        "print(\"Page count = \" seg_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OP0n2RCiSOQM",
        "outputId": "f4f34dff-68de-495e-ccff-a89edd047265"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segment 1 saved to output_segment_1.txt\n",
            "Segment 2 saved to output_segment_2.txt\n",
            "Segment 3 saved to output_segment_3.txt\n",
            "Segment 4 saved to output_segment_4.txt\n",
            "Segment 5 saved to output_segment_5.txt\n",
            "Segment 6 saved to output_segment_6.txt\n",
            "Segment 7 saved to output_segment_7.txt\n",
            "Segment 8 saved to output_segment_8.txt\n",
            "Segment 9 saved to output_segment_9.txt\n",
            "Segment 10 saved to output_segment_10.txt\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "ldm = DiffusionPipeline.from_pretrained(\n",
        "    \"CompVis/ldm-text2im-large-256\"\n",
        "    )\n",
        "\n",
        "output = ldm(\n",
        "     [Prompt],\n",
        "     num_inference_steps=50,\n",
        "     eta=0.3,\n",
        "     guidance_scale=6\n",
        "      )\n",
        "output[\"images\"][0].save(\"image.png\")"
      ],
      "metadata": {
        "id": "y9YplBqlPid1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for Generating PDF with split text\n",
        "\n",
        "class CustomPDF(FPDF):\n",
        "    def header(self):\n",
        "        pass\n",
        "\n",
        "    def footer(self):\n",
        "        pass\n",
        "\n",
        "pdf = CustomPDF()\n",
        "\n",
        "pdf.add_page()\n",
        "pdf.set_font(\"Arial\", size=16)\n",
        "\n",
        "input_files = [f'output_segment_{i + 1}.txt' for i in range(seg_count)]\n",
        "\n",
        "for input_file in input_files:\n",
        "    pdf.add_page()\n",
        "    with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
        "        content = file.readlines()\n",
        "\n",
        "    x_centered = (pdf.w - pdf.get_string_width(content[0].strip())) / 2\n",
        "\n",
        "    for page_num, page_content in enumerate(content, 1):\n",
        "        chunks = [page_content[i:i + 50] for i in range(0, len(page_content), 50)]\n",
        "        image_path = \"image.png\"\n",
        "        pdf.image(image_path, x=10, y=10, w=190)\n",
        "        for chunk in chunks:\n",
        "            pdf.cell(0, 10, txt=chunk.strip(), ln=True, align='C')  # Use ln=True to add a line break\n",
        "\n",
        "pdf.output(\"new2_combined_output.pdf\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0lF9qtU-krDM",
        "outputId": "4932e9dc-4862-4a1e-a940-bec49f61fdeb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fpdf2 in /usr/local/lib/python3.10/dist-packages (2.7.7)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from fpdf2) (0.7.1)\n",
            "Requirement already satisfied: Pillow!=9.2.*,>=6.2.2 in /usr/local/lib/python3.10/dist-packages (from fpdf2) (10.2.0)\n",
            "Requirement already satisfied: fonttools>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from fpdf2) (4.47.2)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3108, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2901, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 441, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 572, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 216, in iter_dependencies\n",
            "    return self._dist.requires(extras)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2821, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3110, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3120, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3173, in __init__\n",
            "    super(Requirement, self).__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/requirements.py\", line 102, in __init__\n",
            "    req = REQUIREMENT.parseString(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 1131, in parse_string\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4959, in parseImpl\n",
            "    loc, tokens = self_expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 856, in _parseNoCache\n",
            "    tokens = fn(instring, tokens_start, ret_tokens)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 291, in wrapper\n",
            "    ret = func(*args[limit:])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/requirements.py\", line 71, in <lambda>\n",
            "    lambda s, l, t: Marker(s[t._original_start : t._original_end])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/markers.py\", line 278, in __init__\n",
            "    self._markers = _coerce_parse_result(MARKER.parseString(marker))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 1131, in parse_string\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 5226, in parseImpl\n",
            "    return super().parseImpl(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4375, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3864, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 825, in _parseNoCache\n",
            "    ret_tokens = ParseResults(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/results.py\", line 136, in __new__\n",
            "    def __new__(cls, toklist=None, name=None, **kwargs):\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 206, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1524, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1624, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1634, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1696, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 968, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 177, in emit\n",
            "    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/console.py\", line 1671, in print\n",
            "    with self:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/console.py\", line 864, in __exit__\n",
            "    self._exit_buffer()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/console.py\", line 822, in _exit_buffer\n",
            "    self._check_buffer()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/console.py\", line 2060, in _check_buffer\n",
            "    self.file.write(text)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-d51a16acf4c2>:17: UserWarning: Substituting font arial by core font helvetica\n",
            "  pdf.set_font(\"Arial\", size=16)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'seg_count' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-d51a16acf4c2>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# List of input text files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0minput_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf'output_segment_{i + 1}.txt'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseg_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m   \u001b[0;31m# Replace with your file paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'seg_count' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic PDF\n",
        "from fpdf import FPDF\n",
        "\n",
        "class CustomPDF(FPDF):\n",
        "    def header(self):\n",
        "        pass\n",
        "\n",
        "    def footer(self):\n",
        "        pass\n",
        "\n",
        "pdf = CustomPDF()\n",
        "\n",
        "pdf.add_page()\n",
        "pdf.set_font(\"Times\", size=12)\n",
        "\n",
        "text_file_path = \"/content/childrens_book.txt\"\n",
        "\n",
        "\n",
        "with open(text_file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    content = file.read()\n",
        "\n",
        "breaks = content.splitlines()\n",
        "\n",
        "for line_num, line in enumerate(breaks, 1):\n",
        "    words = line.split()\n",
        "    chunks = [words[i:i + Word_line] for i in range(0, len(words), Word_line)]\n",
        "\n",
        "    for chunk in chunks:\n",
        "        pdf.cell(0, 10, txt=\" \".join(chunk), ln=True)\n",
        "\n",
        "    if line_num % 4 == 0:\n",
        "        pdf.ln()\n",
        "        pdf.ln()\n",
        "\n",
        "pdf.multi_cell(0, 10, txt=content)\n",
        "\n",
        "pdf.add_page()\n",
        "image_path = \"image.png\"\n",
        "pdf.image(image_path, x=10, y=10, w=190)\n",
        "\n",
        "output_pdf_path = \"output.pdf\"\n",
        "pdf.output(output_pdf_path)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhFXft3k8tME",
        "outputId": "ea10c5c2-8b7c-48eb-8cb8-55f37e8c8b02"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-f7dcbaf83286>:29: DeprecationWarning: The parameter \"txt\" has been renamed to \"text\" in 2.7.6\n",
            "  pdf.cell(0, 10, txt=\" \".join(chunk), ln=True)\n",
            "<ipython-input-18-f7dcbaf83286>:29: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=True use new_x=XPos.LMARGIN, new_y=YPos.NEXT.\n",
            "  pdf.cell(0, 10, txt=\" \".join(chunk), ln=True)\n",
            "<ipython-input-18-f7dcbaf83286>:35: DeprecationWarning: The parameter \"txt\" has been renamed to \"text\" in 2.7.6\n",
            "  pdf.multi_cell(0, 10, txt=content)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding Ryming words for poem\n",
        "\n",
        "if Rhyming == 'y':\n",
        "  import requests\n",
        "  import random\n",
        "\n",
        "  def get_rhyming_words(word):\n",
        "      url = requests.get(f'https://api.datamuse.com/words?rel_rhy={word}')\n",
        "      rhyming_words = [result['word'] for result in url.json()]\n",
        "      return rhyming_words\n",
        "\n",
        "\n",
        "\n",
        "  # Ryming with last word of user input\n",
        "\n",
        "  if len(Prompt) >= Word_line:\n",
        "      chosen_word = Prompt.split()[Word_line - 1]\n",
        "\n",
        "  text_file_path = \"/content/childrens_book.txt\"\n",
        "\n",
        "  def insert_rhyming_words(text, rhyming_words, interval):\n",
        "      words = text.split()\n",
        "      index = (Word_line*2) - 1\n",
        "\n",
        "      while index < len(words):\n",
        "          selected_word = random.choice(rhyming_words)\n",
        "          words.insert(index, selected_word)\n",
        "          index += interval + 1\n",
        "\n",
        "      return \" \".join(words)\n",
        "\n",
        "  with open(text_file_path) as file:\n",
        "      content = file.read()\n",
        "\n",
        "  rhyming_words = get_rhyming_words(chosen_word)\n",
        "\n",
        "  updated_content = insert_rhyming_words(content, rhyming_words, interval=Word_line*2)\n",
        "\n",
        "  with open(text_file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "      file.write(updated_content)\n",
        "\n",
        "  print(chosen_word)\n",
        "else:\n",
        "  print(\"oops\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RgOwJM1DT9D",
        "outputId": "827ce3f4-7d5c-4e2d-ceff-84fa7e99fc79"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rhyming words inserted every 12 words in '/content/childrens_book.txt'.\n",
            "pizza\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3FKT3qHYZRe",
        "outputId": "a6160271-b853-4f30-b9e7-823d912d61e4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7vjKg55e0oE",
        "outputId": "cf7b5429-fdb2-496c-eeee-a669f578c6f9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.10).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone <https://github.com/henrybatson/Level_up_program.git>\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEAx4QoRe4bU",
        "outputId": "0b10669d-0830-4152-ff12-85081caae3c4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: -c: line 1: syntax error near unexpected token `newline'\n",
            "/bin/bash: -c: line 1: `git clone <https://github.com/henrybatson/Level_up_program.git>'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YIvKRsVlfCht"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}